{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ailove/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ailove/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ailove/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# подготовка инструментария\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка данных\n",
    "data = pd.read_csv('eng_train.csv')\n",
    "data = data[['sentiment', 'content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 -> 7\n",
    "r = []\n",
    "for w in data.sentiment:\n",
    "    if w == 'empty' or w == 'neutral':\n",
    "        r.append('empty/neutral')\n",
    "    elif w == 'worry' or w == 'sadness':\n",
    "        r.append('worry/sadness')\n",
    "    elif w == 'surprise' or w == 'relief':\n",
    "        r.append('surprise/relief')\n",
    "    elif w == 'love' or w == 'fun' or w == 'happiness':\n",
    "        r.append('love/fun/happiness')\n",
    "    elif w == 'hate' or w == 'anger':\n",
    "        r.append('hate/anger')\n",
    "    else:\n",
    "        r.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 -> 7\n",
    "r = []\n",
    "for w in data.sentiment:\n",
    "    if w == 'empty' or w == 'neutral':\n",
    "        r.append('neutral')\n",
    "    elif w == 'worry' or w == 'sadness' or w == 'hate' or w == 'anger' or w == 'boredom':\n",
    "        r.append('negative')\n",
    "    else:\n",
    "        r.append('posetive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пытаемся обработать данные вручную (опциональный кусок кода)\n",
    "import re\n",
    "\n",
    "# загрузка лемматизатора\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# загрузка стеммера\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# загрузка списка стопслов\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "processed_content = []\n",
    "\n",
    "for row in data.content:\n",
    "    row = row.lower() #приведение всех текстов к lowercase\n",
    "    row = re.sub('((www\\.[\\s]+)|(https?://[^\\s]+))', 'URL', row) #замена ссылок на URL\n",
    "    row = re.sub('\\_', ' ', row) #удаление подчеркивания\n",
    "    row = re.sub('\\!', ' ATTENTION', row) #замена !\n",
    "    row = re.sub('\\?', ' QUESTION', row) #замена ?\n",
    "    row = re.sub('@[A-Za-z0-9]+', ' ', row) #удаление mentions\n",
    "    row = re.sub('\\W', ' ', row) #удаление символов\n",
    "    row = re.sub('\\_', ' ', row) #удаление подчеркивания\n",
    "    row = re.sub('[\\s]+', ' ', row) #удаление лишних пробелов\n",
    "    row = nltk.word_tokenize(row)\n",
    "    new_row = []\n",
    "    for word in row:\n",
    "        #word = ps.stem(word)\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        if word not in stopWords:\n",
    "            new_row.append(word)\n",
    "    processed_content.append(new_row)\n",
    "    \n",
    "# создание общего словаря слов\n",
    "all_dict = []\n",
    "for n in processed_content:\n",
    "    string = ''\n",
    "    for m in n:\n",
    "        string += m + ' '\n",
    "    all_dict.append(string)\n",
    "    \n",
    "# создание базы {words: [], sentiment: []}\n",
    "raw_data = {}\n",
    "raw_data['content'] = all_dict\n",
    "#raw_data['sentiment'] = data.sentiment\n",
    "raw_data['sentiment'] = r\n",
    "\n",
    "data = pd.DataFrame(raw_data, columns = ['content', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3225     posetive\n",
       "11815    negative\n",
       "7338     negative\n",
       "14980    negative\n",
       "27167    posetive\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data.content, data.sentiment, test_size = 0.2, random_state = 0)\n",
    "X_train.head()\n",
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 20879)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#считаем векторы\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 20879)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 20879)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5341666666666667"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, Y_train)\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB()),])# пайплайн\n",
    "text_clf.fit(X_train, Y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5561666666666667"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier(loss='hinge',alpha=1e-3, random_state=42, max_iter=100, tol=None))])\n",
    "text_clf.fit(X_train, Y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5795"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=20, max_features=\"auto\", random_state=42))])\n",
    "text_clf.fit(X_train, Y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5416666666666666"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ExtraTrees\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0))])\n",
    "text_clf.fit(X_train, Y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
